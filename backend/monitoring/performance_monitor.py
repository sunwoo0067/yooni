#!/usr/bin/env python3
"""
ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
- ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- ì„±ëŠ¥ ë³‘ëª© ì§€ì  ê°ì§€
- ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì¶”ì 
"""

import psutil
import asyncio
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
import psycopg2
from psycopg2.extras import RealDictCursor
import json
import statistics
from collections import deque
from dataclasses import dataclass, asdict
import threading

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤"""
    timestamp: datetime
    cpu_percent: float
    memory_percent: float
    memory_mb: float
    disk_io_read_mb: float
    disk_io_write_mb: float
    network_sent_mb: float
    network_recv_mb: float
    active_connections: int
    query_count: int
    avg_query_time_ms: float
    slow_queries: int
    collection_rate: float  # ìƒí’ˆ/ì´ˆ
    error_rate: float

class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.conn = psycopg2.connect(
            host='localhost',
            port=5434,
            database='yoonni',
            user='postgres',
            password='postgres'
        )
        
        # ë©”íŠ¸ë¦­ ì €ì¥ìš© í (ìµœê·¼ 5ë¶„ ë°ì´í„°)
        self.metrics_queue = deque(maxlen=300)  # 1ì´ˆë§ˆë‹¤ ìˆ˜ì§‘, 5ë¶„ = 300ê°œ
        
        # ì´ì „ ì¸¡ì •ê°’ (ë¸íƒ€ ê³„ì‚°ìš©)
        self.prev_disk_io = psutil.disk_io_counters()
        self.prev_net_io = psutil.net_io_counters()
        self.prev_query_count = 0
        
        # ëª¨ë‹ˆí„°ë§ í”Œë˜ê·¸
        self.monitoring = False
        self.monitor_thread = None
        
    def get_database_metrics(self) -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        try:
            with self.conn.cursor(cursor_factory=RealDictCursor) as cursor:
                # í™œì„± ì—°ê²° ìˆ˜
                cursor.execute("""
                    SELECT COUNT(*) as active_connections 
                    FROM pg_stat_activity 
                    WHERE state = 'active'
                """)
                active_conn = cursor.fetchone()['active_connections']
                
                # ì¿¼ë¦¬ í†µê³„
                cursor.execute("""
                    SELECT 
                        COUNT(*) as query_count,
                        AVG(mean_exec_time) as avg_query_time,
                        COUNT(CASE WHEN mean_exec_time > 1000 THEN 1 END) as slow_queries
                    FROM pg_stat_statements
                    WHERE query NOT LIKE '%pg_stat%'
                """)
                query_stats = cursor.fetchone()
                
                # ìµœê·¼ ìˆ˜ì§‘ ì†ë„
                cursor.execute("""
                    SELECT 
                        AVG(collected_count / NULLIF(processing_time_seconds, 0)) as collection_rate
                    FROM supplier_collection_logs
                    WHERE completed_at > NOW() - INTERVAL '5 minutes'
                    AND status = 'completed'
                """)
                collection_rate = cursor.fetchone()['collection_rate'] or 0
                
                # ì˜¤ë¥˜ìœ¨
                cursor.execute("""
                    SELECT 
                        COUNT(CASE WHEN status = 'failed' THEN 1 END)::float / 
                        NULLIF(COUNT(*), 0) * 100 as error_rate
                    FROM supplier_collection_logs
                    WHERE started_at > NOW() - INTERVAL '1 hour'
                """)
                error_rate = cursor.fetchone()['error_rate'] or 0
                
                return {
                    'active_connections': active_conn,
                    'query_count': query_stats['query_count'] or 0,
                    'avg_query_time_ms': float(query_stats['avg_query_time'] or 0),
                    'slow_queries': query_stats['slow_queries'] or 0,
                    'collection_rate': float(collection_rate),
                    'error_rate': float(error_rate)
                }
                
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return {
                'active_connections': 0,
                'query_count': 0,
                'avg_query_time_ms': 0,
                'slow_queries': 0,
                'collection_rate': 0,
                'error_rate': 0
            }
    
    def collect_metrics(self) -> PerformanceMetrics:
        """ì‹œìŠ¤í…œ ë° ì• í”Œë¦¬ì¼€ì´ì…˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        # CPU ë° ë©”ëª¨ë¦¬
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory = psutil.virtual_memory()
        
        # ë””ìŠ¤í¬ I/O (ë¸íƒ€)
        current_disk = psutil.disk_io_counters()
        disk_read_mb = (current_disk.read_bytes - self.prev_disk_io.read_bytes) / 1024 / 1024
        disk_write_mb = (current_disk.write_bytes - self.prev_disk_io.write_bytes) / 1024 / 1024
        self.prev_disk_io = current_disk
        
        # ë„¤íŠ¸ì›Œí¬ I/O (ë¸íƒ€)
        current_net = psutil.net_io_counters()
        net_sent_mb = (current_net.bytes_sent - self.prev_net_io.bytes_sent) / 1024 / 1024
        net_recv_mb = (current_net.bytes_recv - self.prev_net_io.bytes_recv) / 1024 / 1024
        self.prev_net_io = current_net
        
        # ë°ì´í„°ë² ì´ìŠ¤ ë©”íŠ¸ë¦­
        db_metrics = self.get_database_metrics()
        
        return PerformanceMetrics(
            timestamp=datetime.now(),
            cpu_percent=cpu_percent,
            memory_percent=memory.percent,
            memory_mb=memory.used / 1024 / 1024,
            disk_io_read_mb=max(0, disk_read_mb),
            disk_io_write_mb=max(0, disk_write_mb),
            network_sent_mb=max(0, net_sent_mb),
            network_recv_mb=max(0, net_recv_mb),
            **db_metrics
        )
    
    def analyze_performance(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë¶„ì„ ë° ë³‘ëª© ì§€ì  ê°ì§€"""
        if len(self.metrics_queue) < 10:
            return {"status": "insufficient_data"}
        
        recent_metrics = list(self.metrics_queue)[-60:]  # ìµœê·¼ 1ë¶„
        
        # í‰ê·  ê³„ì‚°
        avg_cpu = statistics.mean(m.cpu_percent for m in recent_metrics)
        avg_memory = statistics.mean(m.memory_percent for m in recent_metrics)
        avg_query_time = statistics.mean(m.avg_query_time_ms for m in recent_metrics)
        avg_collection_rate = statistics.mean(m.collection_rate for m in recent_metrics)
        
        # ë³‘ëª© ì§€ì  ê°ì§€
        bottlenecks = []
        recommendations = []
        
        # CPU ë³‘ëª©
        if avg_cpu > 80:
            bottlenecks.append("CPU")
            recommendations.append("CPU ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤. ë³‘ë ¬ ì²˜ë¦¬ ë˜ëŠ” ìŠ¤ì¼€ì¼ ì•„ì›ƒì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        # ë©”ëª¨ë¦¬ ë³‘ëª©
        if avg_memory > 85:
            bottlenecks.append("Memory")
            recommendations.append("ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤. ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì´ê±°ë‚˜ ë©”ëª¨ë¦¬ë¥¼ ì¦ì„¤í•˜ì„¸ìš”.")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ë³‘ëª©
        if avg_query_time > 100:
            bottlenecks.append("Database")
            recommendations.append("ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ì´ ê¹ë‹ˆë‹¤. ì¸ë±ìŠ¤ ìµœì í™”ë‚˜ ì¿¼ë¦¬ íŠœë‹ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ìˆ˜ì§‘ ì†ë„ ì €í•˜
        if avg_collection_rate < 50 and avg_collection_rate > 0:
            bottlenecks.append("Collection")
            recommendations.append("ìˆ˜ì§‘ ì†ë„ê°€ ëŠë¦½ë‹ˆë‹¤. API ì—°ê²° ìƒíƒœë‚˜ ë„¤íŠ¸ì›Œí¬ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        
        # ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚° (0-100)
        performance_score = 100
        performance_score -= max(0, avg_cpu - 50) * 0.5  # CPU 50% ì´ìƒì‹œ ê°ì 
        performance_score -= max(0, avg_memory - 50) * 0.3  # ë©”ëª¨ë¦¬ 50% ì´ìƒì‹œ ê°ì 
        performance_score -= max(0, avg_query_time - 50) * 0.2  # ì¿¼ë¦¬ 50ms ì´ìƒì‹œ ê°ì 
        performance_score = max(0, min(100, performance_score))
        
        return {
            "status": "analyzed",
            "performance_score": round(performance_score, 1),
            "metrics": {
                "avg_cpu_percent": round(avg_cpu, 1),
                "avg_memory_percent": round(avg_memory, 1),
                "avg_query_time_ms": round(avg_query_time, 1),
                "avg_collection_rate": round(avg_collection_rate, 1)
            },
            "bottlenecks": bottlenecks,
            "recommendations": recommendations,
            "analyzed_at": datetime.now().isoformat()
        }
    
    def save_metrics_to_db(self, metrics: PerformanceMetrics):
        """ë©”íŠ¸ë¦­ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        try:
            with self.conn.cursor() as cursor:
                cursor.execute("""
                    INSERT INTO performance_metrics 
                    (timestamp, metrics_data, performance_score)
                    VALUES (%s, %s, %s)
                """, (
                    metrics.timestamp,
                    json.dumps(asdict(metrics), default=str),
                    self.analyze_performance().get('performance_score', 0)
                ))
            self.conn.commit()
        except Exception as e:
            logger.error(f"ë©”íŠ¸ë¦­ ì €ì¥ ì‹¤íŒ¨: {e}")
            self.conn.rollback()
    
    def monitor_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        logger.info("ğŸš€ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        
        while self.monitoring:
            try:
                # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                metrics = self.collect_metrics()
                self.metrics_queue.append(metrics)
                
                # 10ì´ˆë§ˆë‹¤ DB ì €ì¥
                if len(self.metrics_queue) % 10 == 0:
                    self.save_metrics_to_db(metrics)
                
                # ì„±ëŠ¥ ì €í•˜ ê°ì§€
                analysis = self.analyze_performance()
                if analysis.get('performance_score', 100) < 70:
                    logger.warning(f"âš ï¸ ì„±ëŠ¥ ì €í•˜ ê°ì§€: {analysis['performance_score']}ì ")
                    logger.warning(f"   ë³‘ëª©: {', '.join(analysis['bottlenecks'])}")
                
                time.sleep(1)  # 1ì´ˆ ëŒ€ê¸°
                
            except Exception as e:
                logger.error(f"ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                time.sleep(5)
    
    def start(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if not self.monitoring:
            self.monitoring = True
            self.monitor_thread = threading.Thread(target=self.monitor_loop)
            self.monitor_thread.daemon = True
            self.monitor_thread.start()
            logger.info("âœ… ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘ë¨")
    
    def stop(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        if self.monitoring:
            self.monitoring = False
            if self.monitor_thread:
                self.monitor_thread.join()
            logger.info("ğŸ›‘ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€ë¨")
    
    def get_realtime_metrics(self) -> Dict[str, Any]:
        """ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        if not self.metrics_queue:
            return {"error": "No metrics available"}
        
        latest = self.metrics_queue[-1]
        return {
            "timestamp": latest.timestamp.isoformat(),
            "cpu_percent": latest.cpu_percent,
            "memory_percent": latest.memory_percent,
            "memory_mb": round(latest.memory_mb, 1),
            "disk_io": {
                "read_mb_s": round(latest.disk_io_read_mb, 2),
                "write_mb_s": round(latest.disk_io_write_mb, 2)
            },
            "network": {
                "sent_mb_s": round(latest.network_sent_mb, 2),
                "recv_mb_s": round(latest.network_recv_mb, 2)
            },
            "database": {
                "active_connections": latest.active_connections,
                "avg_query_time_ms": round(latest.avg_query_time_ms, 1),
                "slow_queries": latest.slow_queries
            },
            "application": {
                "collection_rate": round(latest.collection_rate, 1),
                "error_rate": round(latest.error_rate, 1)
            }
        }

def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    monitor = PerformanceMonitor()
    monitor.start()
    
    try:
        # 30ì´ˆ ë™ì•ˆ ëª¨ë‹ˆí„°ë§
        for i in range(30):
            time.sleep(1)
            if i % 5 == 0:
                metrics = monitor.get_realtime_metrics()
                print(f"\nğŸ“Š ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­:")
                print(f"   CPU: {metrics['cpu_percent']}%")
                print(f"   Memory: {metrics['memory_percent']}%")
                print(f"   Collection Rate: {metrics['application']['collection_rate']} items/sec")
                
                if i == 15:
                    analysis = monitor.analyze_performance()
                    print(f"\nğŸ¯ ì„±ëŠ¥ ë¶„ì„:")
                    print(f"   ì ìˆ˜: {analysis.get('performance_score', 'N/A')}ì ")
                    if 'bottlenecks' in analysis:
                        print(f"   ë³‘ëª©: {', '.join(analysis['bottlenecks'])}")
                
    finally:
        monitor.stop()

if __name__ == "__main__":
    main()
#!/usr/bin/env python3
"""
ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ë„êµ¬
- ì¸ë±ìŠ¤ ìµœì í™”
- íŒŒí‹°ì…”ë‹ êµ¬í˜„
- ì¿¼ë¦¬ ì„±ëŠ¥ ë¶„ì„
- ìë™ VACUUM ë° ANALYZE
"""

import psycopg2
from psycopg2.extras import RealDictCursor
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any
import json

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DatabaseOptimizer:
    """ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.conn = psycopg2.connect(
            host='localhost',
            port=5434,
            database='yoonni',
            user='postgres',
            password='postgres'
        )
        self.conn.autocommit = True
    
    def analyze_table_sizes(self) -> List[Dict[str, Any]]:
        """í…Œì´ë¸” í¬ê¸° ë¶„ì„"""
        with self.conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute("""
                SELECT 
                    schemaname,
                    tablename,
                    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size,
                    pg_total_relation_size(schemaname||'.'||tablename) AS size_bytes,
                    n_live_tup AS row_count,
                    n_dead_tup AS dead_rows,
                    last_vacuum,
                    last_autovacuum,
                    last_analyze,
                    last_autoanalyze
                FROM pg_stat_user_tables
                ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC
                LIMIT 20
            """)
            
            tables = cursor.fetchall()
            
            logger.info("ğŸ“Š í…Œì´ë¸” í¬ê¸° ë¶„ì„:")
            for table in tables[:10]:
                logger.info(f"   {table['tablename']}: {table['size']} "
                          f"(í–‰: {table['row_count']:,}, ë°ë“œ: {table['dead_rows']:,})")
            
            return tables
    
    def optimize_indexes(self):
        """ì¸ë±ìŠ¤ ìµœì í™”"""
        logger.info("ğŸ”§ ì¸ë±ìŠ¤ ìµœì í™” ì‹œì‘...")
        
        # 1. ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ì¸ë±ìŠ¤ ì°¾ê¸°
        with self.conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute("""
                SELECT 
                    schemaname,
                    tablename,
                    indexname,
                    pg_size_pretty(pg_relation_size(indexrelid)) AS index_size,
                    idx_scan,
                    idx_tup_read,
                    idx_tup_fetch
                FROM pg_stat_user_indexes
                WHERE idx_scan = 0
                AND indexrelname NOT LIKE 'pg_toast%'
                ORDER BY pg_relation_size(indexrelid) DESC
            """)
            
            unused_indexes = cursor.fetchall()
            
            if unused_indexes:
                logger.warning(f"âš ï¸ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ì¸ë±ìŠ¤ {len(unused_indexes)}ê°œ ë°œê²¬:")
                for idx in unused_indexes[:5]:
                    logger.warning(f"   - {idx['indexname']} on {idx['tablename']} ({idx['index_size']})")
            
            # 2. ëˆ„ë½ëœ ì¸ë±ìŠ¤ ì œì•ˆ
            self._suggest_missing_indexes()
            
            # 3. ë³µí•© ì¸ë±ìŠ¤ ìµœì í™”
            self._optimize_composite_indexes()
    
    def _suggest_missing_indexes(self):
        """ëˆ„ë½ëœ ì¸ë±ìŠ¤ ì œì•ˆ"""
        suggestions = []
        
        # supplier_products í…Œì´ë¸” ìµœì í™”
        suggestions.append(
            "CREATE INDEX IF NOT EXISTS idx_supplier_products_status_category "
            "ON supplier_products(status, category) WHERE status = 'active';"
        )
        
        suggestions.append(
            "CREATE INDEX IF NOT EXISTS idx_supplier_products_supplier_collected "
            "ON supplier_products(supplier_id, collected_at DESC);"
        )
        
        # supplier_collection_logs ìµœì í™”
        suggestions.append(
            "CREATE INDEX IF NOT EXISTS idx_collection_logs_supplier_started "
            "ON supplier_collection_logs(supplier_id, started_at DESC);"
        )
        
        # AI ë¶„ì„ í…Œì´ë¸” ìµœì í™”
        suggestions.append(
            "CREATE INDEX IF NOT EXISTS idx_product_analysis_updated "
            "ON supplier_product_analysis(updated_at DESC) "
            "WHERE analysis_type = 'price_demand';"
        )
        
        logger.info("ğŸ’¡ ì¸ë±ìŠ¤ ìƒì„± ì œì•ˆ:")
        for suggestion in suggestions:
            logger.info(f"   {suggestion[:60]}...")
            try:
                with self.conn.cursor() as cursor:
                    cursor.execute(suggestion)
                logger.info("   âœ… ìƒì„± ì™„ë£Œ")
            except Exception as e:
                logger.error(f"   âŒ ì‹¤íŒ¨: {e}")
    
    def _optimize_composite_indexes(self):
        """ë³µí•© ì¸ë±ìŠ¤ ìµœì í™”"""
        # ìì£¼ ì‚¬ìš©ë˜ëŠ” ì¿¼ë¦¬ íŒ¨í„´ì— ëŒ€í•œ ë³µí•© ì¸ë±ìŠ¤
        with self.conn.cursor() as cursor:
            # JSONB ì¸ë±ìŠ¤ ìµœì í™”
            cursor.execute("""
                CREATE INDEX IF NOT EXISTS idx_supplier_products_jsonb_gin 
                ON supplier_products USING gin (product_data);
            """)
            
            # ë¶€ë¶„ ì¸ë±ìŠ¤ ìƒì„± (í™œì„± ìƒí’ˆë§Œ)
            cursor.execute("""
                CREATE INDEX IF NOT EXISTS idx_active_products_category 
                ON supplier_products(category, price) 
                WHERE status = 'active';
            """)
            
            logger.info("âœ… ë³µí•© ì¸ë±ìŠ¤ ìµœì í™” ì™„ë£Œ")
    
    def setup_partitioning(self):
        """í…Œì´ë¸” íŒŒí‹°ì…”ë‹ ì„¤ì •"""
        logger.info("ğŸ“… íŒŒí‹°ì…”ë‹ ì„¤ì • ì‹œì‘...")
        
        # supplier_collection_logs ì›”ë³„ íŒŒí‹°ì…”ë‹
        current_month = datetime.now().strftime('%Y_%m')
        next_month = (datetime.now() + timedelta(days=32)).strftime('%Y_%m')
        
        try:
            with self.conn.cursor() as cursor:
                # íŒŒí‹°ì…˜ í…Œì´ë¸” ìƒì„±
                cursor.execute(f"""
                    CREATE TABLE IF NOT EXISTS supplier_collection_logs_{current_month} 
                    PARTITION OF supplier_collection_logs
                    FOR VALUES FROM ('{datetime.now().strftime('%Y-%m-01')}') 
                    TO ('{(datetime.now() + timedelta(days=32)).strftime('%Y-%m-01')}');
                """)
                
                # ë‹¤ìŒ ë‹¬ íŒŒí‹°ì…˜ ë¯¸ë¦¬ ìƒì„±
                cursor.execute(f"""
                    CREATE TABLE IF NOT EXISTS supplier_collection_logs_{next_month} 
                    PARTITION OF supplier_collection_logs
                    FOR VALUES FROM ('{(datetime.now() + timedelta(days=32)).strftime('%Y-%m-01')}') 
                    TO ('{(datetime.now() + timedelta(days=62)).strftime('%Y-%m-01')}');
                """)
                
                logger.info(f"âœ… íŒŒí‹°ì…˜ ìƒì„± ì™„ë£Œ: {current_month}, {next_month}")
                
        except Exception as e:
            if "is not partitioned" in str(e):
                logger.warning("âš ï¸ íŒŒí‹°ì…”ë‹ì„ ìœ„í•´ì„œëŠ” í…Œì´ë¸” ì¬ìƒì„±ì´ í•„ìš”í•©ë‹ˆë‹¤")
                self._create_partitioned_table_script()
            else:
                logger.error(f"íŒŒí‹°ì…”ë‹ ì‹¤íŒ¨: {e}")
    
    def _create_partitioned_table_script(self):
        """íŒŒí‹°ì…˜ í…Œì´ë¸” ìƒì„± ìŠ¤í¬ë¦½íŠ¸ ì¶œë ¥"""
        script = """
-- íŒŒí‹°ì…˜ í…Œì´ë¸”ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸
-- ì£¼ì˜: ì‹¤í–‰ ì „ ë°±ì—… í•„ìˆ˜!

-- 1. ìƒˆë¡œìš´ íŒŒí‹°ì…˜ í…Œì´ë¸” ìƒì„±
CREATE TABLE supplier_collection_logs_new (
    LIKE supplier_collection_logs INCLUDING ALL
) PARTITION BY RANGE (started_at);

-- 2. ì›”ë³„ íŒŒí‹°ì…˜ ìƒì„± (ìµœê·¼ 6ê°œì›”)
CREATE TABLE supplier_collection_logs_2025_01 PARTITION OF supplier_collection_logs_new
    FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');

CREATE TABLE supplier_collection_logs_2025_02 PARTITION OF supplier_collection_logs_new
    FOR VALUES FROM ('2025-02-01') TO ('2025-03-01');

-- ì´ì „ ë‹¬ë“¤ë„ ë™ì¼í•˜ê²Œ...

-- 3. ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜
INSERT INTO supplier_collection_logs_new SELECT * FROM supplier_collection_logs;

-- 4. í…Œì´ë¸” êµì²´
ALTER TABLE supplier_collection_logs RENAME TO supplier_collection_logs_old;
ALTER TABLE supplier_collection_logs_new RENAME TO supplier_collection_logs;

-- 5. ì´ì „ í…Œì´ë¸” ì‚­ì œ (í™•ì¸ í›„)
-- DROP TABLE supplier_collection_logs_old;
        """
        
        logger.info("ğŸ“ íŒŒí‹°ì…˜ í…Œì´ë¸” ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸:")
        print(script)
    
    def analyze_slow_queries(self) -> List[Dict[str, Any]]:
        """ëŠë¦° ì¿¼ë¦¬ ë¶„ì„"""
        logger.info("ğŸŒ ëŠë¦° ì¿¼ë¦¬ ë¶„ì„...")
        
        with self.conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute("""
                SELECT 
                    query,
                    calls,
                    total_exec_time,
                    mean_exec_time,
                    stddev_exec_time,
                    rows
                FROM pg_stat_statements
                WHERE mean_exec_time > 100  -- 100ms ì´ìƒ
                AND query NOT LIKE '%pg_stat%'
                ORDER BY mean_exec_time DESC
                LIMIT 10
            """)
            
            slow_queries = cursor.fetchall()
            
            if slow_queries:
                logger.warning(f"âš ï¸ ëŠë¦° ì¿¼ë¦¬ {len(slow_queries)}ê°œ ë°œê²¬:")
                for i, query in enumerate(slow_queries[:5], 1):
                    logger.warning(f"\n{i}. í‰ê·  ì‹¤í–‰ì‹œê°„: {query['mean_exec_time']:.1f}ms")
                    logger.warning(f"   í˜¸ì¶œ íšŸìˆ˜: {query['calls']}")
                    logger.warning(f"   ì¿¼ë¦¬: {query['query'][:100]}...")
            
            return slow_queries
    
    def vacuum_and_analyze(self, full_vacuum: bool = False):
        """VACUUM ë° ANALYZE ì‹¤í–‰"""
        logger.info("ğŸ§¹ VACUUM ë° ANALYZE ì‹¤í–‰...")
        
        tables = [
            'supplier_products',
            'supplier_collection_logs',
            'supplier_product_analysis',
            'suppliers',
            'supplier_configs'
        ]
        
        for table in tables:
            try:
                with self.conn.cursor() as cursor:
                    if full_vacuum:
                        logger.info(f"   VACUUM FULL {table}...")
                        cursor.execute(f"VACUUM FULL ANALYZE {table}")
                    else:
                        logger.info(f"   VACUUM ANALYZE {table}...")
                        cursor.execute(f"VACUUM ANALYZE {table}")
                    
                logger.info(f"   âœ… {table} ì™„ë£Œ")
                
            except Exception as e:
                logger.error(f"   âŒ {table} ì‹¤íŒ¨: {e}")
    
    def get_optimization_report(self) -> Dict[str, Any]:
        """ìµœì í™” ë³´ê³ ì„œ ìƒì„±"""
        logger.info("ğŸ“Š ìµœì í™” ë³´ê³ ì„œ ìƒì„±...")
        
        report = {
            "timestamp": datetime.now().isoformat(),
            "database_size": self._get_database_size(),
            "table_sizes": self.analyze_table_sizes()[:5],
            "index_usage": self._get_index_usage_stats(),
            "cache_hit_ratio": self._get_cache_hit_ratio(),
            "connection_stats": self._get_connection_stats(),
            "recommendations": []
        }
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        if report['cache_hit_ratio'] < 90:
            report['recommendations'].append(
                "ìºì‹œ íˆíŠ¸ìœ¨ì´ ë‚®ìŠµë‹ˆë‹¤. shared_buffers ì¦ê°€ë¥¼ ê³ ë ¤í•˜ì„¸ìš”."
            )
        
        if report['connection_stats']['idle_in_transaction'] > 5:
            report['recommendations'].append(
                "idle in transaction ì—°ê²°ì´ ë§ìŠµë‹ˆë‹¤. íŠ¸ëœì­ì…˜ ê´€ë¦¬ë¥¼ í™•ì¸í•˜ì„¸ìš”."
            )
        
        # í° í…Œì´ë¸” í™•ì¸
        for table in report['table_sizes']:
            if table['dead_rows'] > table['row_count'] * 0.1:
                report['recommendations'].append(
                    f"{table['tablename']} í…Œì´ë¸”ì˜ dead rowsê°€ ë§ìŠµë‹ˆë‹¤. VACUUMì„ ì‹¤í–‰í•˜ì„¸ìš”."
                )
        
        return report
    
    def _get_database_size(self) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸° ì¡°íšŒ"""
        with self.conn.cursor() as cursor:
            cursor.execute("SELECT pg_size_pretty(pg_database_size('yoonni'))")
            return cursor.fetchone()[0]
    
    def _get_index_usage_stats(self) -> Dict[str, Any]:
        """ì¸ë±ìŠ¤ ì‚¬ìš© í†µê³„"""
        with self.conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute("""
                SELECT 
                    COUNT(*) as total_indexes,
                    COUNT(CASE WHEN idx_scan = 0 THEN 1 END) as unused_indexes,
                    AVG(idx_scan) as avg_scans
                FROM pg_stat_user_indexes
            """)
            return cursor.fetchone()
    
    def _get_cache_hit_ratio(self) -> float:
        """ìºì‹œ íˆíŠ¸ìœ¨ ê³„ì‚°"""
        with self.conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute("""
                SELECT 
                    sum(heap_blks_hit) / (sum(heap_blks_hit) + sum(heap_blks_read)) * 100 
                    as cache_hit_ratio
                FROM pg_statio_user_tables
            """)
            result = cursor.fetchone()
            return float(result['cache_hit_ratio'] or 0)
    
    def _get_connection_stats(self) -> Dict[str, int]:
        """ì—°ê²° í†µê³„"""
        with self.conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute("""
                SELECT 
                    state,
                    COUNT(*) as count
                FROM pg_stat_activity
                GROUP BY state
            """)
            
            stats = {row['state']: row['count'] for row in cursor.fetchall() if row['state']}
            stats['total'] = sum(stats.values())
            return stats

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    optimizer = DatabaseOptimizer()
    
    # 1. ìµœì í™” ë³´ê³ ì„œ
    report = optimizer.get_optimization_report()
    print("\nğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ë³´ê³ ì„œ")
    print(f"ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸°: {report['database_size']}")
    print(f"ìºì‹œ íˆíŠ¸ìœ¨: {report['cache_hit_ratio']:.1f}%")
    print(f"ì´ ì¸ë±ìŠ¤: {report['index_usage']['total_indexes']}")
    print(f"ë¯¸ì‚¬ìš© ì¸ë±ìŠ¤: {report['index_usage']['unused_indexes']}")
    
    if report['recommendations']:
        print("\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        for rec in report['recommendations']:
            print(f"   - {rec}")
    
    # 2. ì¸ë±ìŠ¤ ìµœì í™”
    optimizer.optimize_indexes()
    
    # 3. ëŠë¦° ì¿¼ë¦¬ ë¶„ì„
    optimizer.analyze_slow_queries()
    
    # 4. VACUUM ì‹¤í–‰ (í•„ìš”ì‹œ)
    # optimizer.vacuum_and_analyze()

if __name__ == "__main__":
    main()
#!/usr/bin/env python3
"""
MLOps í”Œë«í¼ ì‚¬ìš© ì˜ˆì œ
"""
import asyncio
import numpy as np
from datetime import datetime, timedelta
import logging
from model_manager import (
    ModelManager, ModelType, ModelStatus,
    ModelRegistry, ModelMonitor, ExperimentTracker
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


async def basic_model_lifecycle_demo():
    """ê¸°ë³¸ ëª¨ë¸ ìƒëª…ì£¼ê¸° ë°ëª¨"""
    print("=== ëª¨ë¸ ìƒëª…ì£¼ê¸° ê´€ë¦¬ ë°ëª¨ ===\n")
    
    # ëª¨ë¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”
    model_manager = ModelManager({
        'registry_path': './model_registry_demo',
        'tracking_uri': './mlruns_demo'
    })
    
    # 1. ëª¨ë¸ í•™ìŠµ ë° ë“±ë¡
    print("1. ëª¨ë¸ í•™ìŠµ ë° ë“±ë¡")
    model_id = "sales_predictor"
    version = await model_manager.train_and_register_model(
        model_id=model_id,
        model_type=ModelType.TIME_SERIES,
        train_data=None,  # ì‹¤ì œë¡œëŠ” í•™ìŠµ ë°ì´í„°
        params={
            'algorithm': 'LSTM',
            'epochs': 100,
            'learning_rate': 0.001,
            'batch_size': 32
        }
    )
    print(f"âœ“ ëª¨ë¸ {model_id} ë²„ì „ {version} ë“±ë¡ ì™„ë£Œ\n")
    
    # 2. ëª¨ë¸ ë°°í¬
    print("2. ëª¨ë¸ ë°°í¬")
    deployment = await model_manager.deploy_model(
        model_id=model_id,
        version=version,
        config={
            'deployment_type': 'api',
            'port': 8001,
            'replicas': 2,
            'memory_limit': '2Gi',
            'cpu_limit': '1000m'
        }
    )
    print(f"âœ“ ë°°í¬ ì™„ë£Œ: {deployment['deployment_id']}")
    print(f"  ì—”ë“œí¬ì¸íŠ¸: {deployment.get('endpoint', 'N/A')}\n")
    
    # 3. ëª¨ë¸ ìƒíƒœ í™•ì¸
    print("3. ëª¨ë¸ ìƒíƒœ í™•ì¸")
    status = model_manager.get_model_status(model_id)
    print(f"ëª¨ë¸ ID: {status['model_id']}")
    print(f"ë²„ì „ ìˆ˜: {len(status['versions'])}")
    print(f"í™œì„± ë°°í¬: {len(status['deployments'])}")
    
    # 4. ëª¨ë‹ˆí„°ë§ ì‹œë®¬ë ˆì´ì…˜
    print("\n4. ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§")
    for i in range(3):
        # ì˜ˆì¸¡ ì‹œë®¬ë ˆì´ì…˜
        predictions = np.random.rand(50) * 100
        actuals = predictions + np.random.normal(0, 5, 50)
        
        metrics = await model_manager.monitor.monitor_performance(
            model_id=model_id,
            predictions=predictions,
            actuals=actuals,
            model_type=ModelType.TIME_SERIES
        )
        
        print(f"  ë°˜ë³µ {i+1} - RMSE: {metrics['rmse']:.2f}, MAE: {metrics['mae']:.2f}")
        await asyncio.sleep(1)
    
    # 5. ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸
    print("\n5. ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸")
    report = model_manager.monitor.get_monitoring_report(model_id, hours=1)
    if 'metrics_summary' in report:
        print("ë©”íŠ¸ë¦­ ìš”ì•½:")
        for metric, stats in report['metrics_summary'].items():
            print(f"  {metric}: í‰ê· ={stats['mean']:.3f}, í‘œì¤€í¸ì°¨={stats['std']:.3f}")


async def experiment_tracking_demo():
    """ì‹¤í—˜ ì¶”ì  ë°ëª¨"""
    print("\n=== ì‹¤í—˜ ì¶”ì  ë°ëª¨ ===\n")
    
    tracker = ExperimentTracker('./mlruns_experiment_demo')
    
    # ì‹¤í—˜ ìƒì„±
    experiment_id = tracker.create_experiment(
        name="hyperparameter_tuning",
        description="í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í—˜"
    )
    
    # ì—¬ëŸ¬ ì‹¤í–‰ìœ¼ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰
    best_run = None
    best_metric = float('inf')
    
    hyperparameters = [
        {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 50},
        {'learning_rate': 0.01, 'batch_size': 64, 'epochs': 50},
        {'learning_rate': 0.005, 'batch_size': 32, 'epochs': 100}
    ]
    
    for i, params in enumerate(hyperparameters):
        print(f"\nì‹¤í–‰ {i+1}: {params}")
        
        # ì‹¤í–‰ ì‹œì‘
        run_id = tracker.start_run(experiment_id, f"run_{i+1}")
        
        # íŒŒë¼ë¯¸í„° ë¡œê¹…
        tracker.log_params(params)
        
        # í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜ (ì—í­ë³„ ë©”íŠ¸ë¦­)
        for epoch in range(5):  # ê°„ë‹¨íˆ 5 ì—í­ë§Œ
            # ë©”íŠ¸ë¦­ ì‹œë®¬ë ˆì´ì…˜
            loss = np.random.exponential(0.1) * (1 - epoch/10)
            accuracy = 0.8 + epoch * 0.02 + np.random.normal(0, 0.01)
            
            tracker.log_metrics({
                'loss': loss,
                'accuracy': accuracy
            }, step=epoch)
            
            print(f"  Epoch {epoch+1}: loss={loss:.3f}, accuracy={accuracy:.3f}")
        
        # ìµœì¢… ë©”íŠ¸ë¦­
        final_loss = loss
        tracker.log_metrics({
            'final_loss': final_loss,
            'final_accuracy': accuracy
        })
        
        # ìµœê³  ì„±ëŠ¥ ì¶”ì 
        if final_loss < best_metric:
            best_metric = final_loss
            best_run = {
                'run_id': run_id,
                'params': params,
                'loss': final_loss
            }
        
        tracker.end_run()
    
    print(f"\nìµœê³  ì„±ëŠ¥ ì‹¤í–‰:")
    print(f"  Run ID: {best_run['run_id']}")
    print(f"  íŒŒë¼ë¯¸í„°: {best_run['params']}")
    print(f"  Loss: {best_run['loss']:.3f}")


async def model_versioning_demo():
    """ëª¨ë¸ ë²„ì „ ê´€ë¦¬ ë°ëª¨"""
    print("\n=== ëª¨ë¸ ë²„ì „ ê´€ë¦¬ ë°ëª¨ ===\n")
    
    registry = ModelRegistry('./model_registry_version_demo')
    
    # ì—¬ëŸ¬ ë²„ì „ ë“±ë¡
    model_id = "customer_churn_model"
    versions = ["1.0.0", "1.0.1", "1.1.0", "2.0.0"]
    
    for i, version in enumerate(versions):
        print(f"ë²„ì „ {version} ë“±ë¡ ì¤‘...")
        
        # ëª¨ë¸ íŒŒì¼ ìƒì„± (ì‹œë®¬ë ˆì´ì…˜)
        import pickle
        import os
        
        model_dir = f"./temp_models_{model_id}"
        os.makedirs(model_dir, exist_ok=True)
        model_path = f"{model_dir}/model_v{version}.pkl"
        
        with open(model_path, 'wb') as f:
            pickle.dump({
                'version': version,
                'accuracy': 0.85 + i * 0.02,
                'created_at': datetime.now()
            }, f)
        
        # ëª¨ë¸ ë“±ë¡
        model_version = registry.register_model(
            model_id=model_id,
            version=version,
            model_path=model_path,
            metadata={
                'algorithm': 'XGBoost',
                'accuracy': 0.85 + i * 0.02,
                'f1_score': 0.82 + i * 0.02,
                'training_data': 'customer_data_v2',
                'features': ['recency', 'frequency', 'monetary', 'tenure']
            }
        )
        
        # ë²„ì „ 2.0.0ì„ í”„ë¡œë•ì…˜ìœ¼ë¡œ ìŠ¹ê²©
        if version == "2.0.0":
            model_version.promote_to_production()
            print(f"  âœ“ ë²„ì „ {version} í”„ë¡œë•ì…˜ìœ¼ë¡œ ìŠ¹ê²©")
        
        await asyncio.sleep(0.5)
    
    # ëª¨ë¸ ëª©ë¡ í™•ì¸
    print("\në“±ë¡ëœ ëª¨ë¸ ë²„ì „:")
    models = registry.list_models()
    for model in models:
        print(f"  - {model['model_id']} v{model['version']}: "
              f"ìƒíƒœ={model['status']}, "
              f"ì •í™•ë„={model['metadata'].get('accuracy', 'N/A')}")
    
    # í”„ë¡œë•ì…˜ ëª¨ë¸ ì¡°íšŒ
    print("\ní”„ë¡œë•ì…˜ ëª¨ë¸ ì¡°íšŒ:")
    prod_model = registry.get_model(model_id)  # ë²„ì „ ì—†ì´ ì¡°íšŒí•˜ë©´ í”„ë¡œë•ì…˜ ë²„ì „
    print(f"  ëª¨ë¸: {prod_model.model_id}")
    print(f"  ë²„ì „: {prod_model.version}")
    print(f"  ìƒíƒœ: {prod_model.status.value}")


async def monitoring_alerts_demo():
    """ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ë°ëª¨"""
    print("\n=== ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ë°ëª¨ ===\n")
    
    monitor = ModelMonitor()
    
    # ì•Œë¦¼ í•¸ë“¤ëŸ¬ ì„¤ì •
    alerts_received = []
    
    async def alert_handler(model_id: str, alert: Dict[str, Any]):
        alerts_received.append(alert)
        print(f"\nğŸš¨ ì•Œë¦¼ ë°œìƒ!")
        print(f"  ëª¨ë¸: {model_id}")
        print(f"  ìœ í˜•: {alert['type']}")
        print(f"  ì‹¬ê°ë„: {alert['severity']}")
        print(f"  ë©”ì‹œì§€: {alert['message']}")
    
    monitor.add_alert_callback(alert_handler)
    
    # ì •ìƒ ì„±ëŠ¥ ì‹œë®¬ë ˆì´ì…˜
    model_id = "recommendation_model"
    print("1. ì •ìƒ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§")
    
    for i in range(3):
        predictions = np.random.rand(100)
        actuals = predictions + np.random.normal(0, 0.05, 100)  # ì‘ì€ ì˜¤ì°¨
        
        metrics = await monitor.monitor_performance(
            model_id=model_id,
            predictions=predictions,
            actuals=actuals,
            model_type=ModelType.REGRESSION
        )
        print(f"  ë°˜ë³µ {i+1}: RMSE={metrics['rmse']:.3f}")
        await asyncio.sleep(0.5)
    
    # ì„±ëŠ¥ ì €í•˜ ì‹œë®¬ë ˆì´ì…˜
    print("\n2. ì„±ëŠ¥ ì €í•˜ ì‹œë®¬ë ˆì´ì…˜")
    
    # ë¨¼ì € ì¢‹ì€ ì„±ëŠ¥ ê¸°ë¡
    good_predictions = np.random.rand(100)
    good_actuals = good_predictions + np.random.normal(0, 0.05, 100)
    
    await monitor.monitor_performance(
        model_id=model_id,
        predictions=good_predictions,
        actuals=good_actuals,
        model_type=ModelType.CLASSIFICATION
    )
    
    # ì„±ëŠ¥ ì €í•˜
    bad_predictions = np.random.rand(100)
    bad_actuals = np.ones(100) - bad_predictions  # ë°˜ëŒ€ ì˜ˆì¸¡
    
    await monitor.monitor_performance(
        model_id=model_id,
        predictions=bad_predictions,
        actuals=bad_actuals,
        model_type=ModelType.CLASSIFICATION
    )
    
    # ë°ì´í„° ë“œë¦¬í”„íŠ¸ ì‹œë®¬ë ˆì´ì…˜
    print("\n3. ë°ì´í„° ë“œë¦¬í”„íŠ¸ ì‹œë®¬ë ˆì´ì…˜")
    
    # ë¶„í¬ê°€ ë‹¤ë¥¸ ë°ì´í„°
    drift_predictions = np.random.normal(0.8, 0.1, 100)  # ë‹¤ë¥¸ í‰ê· 
    drift_actuals = np.random.normal(0.2, 0.1, 100)  # ë§¤ìš° ë‹¤ë¥¸ ë¶„í¬
    
    metrics = await monitor.monitor_performance(
        model_id=model_id,
        predictions=drift_predictions,
        actuals=drift_actuals,
        model_type=ModelType.REGRESSION
    )
    
    print(f"\nì´ {len(alerts_received)}ê°œì˜ ì•Œë¦¼ ë°œìƒ")


async def deployment_strategies_demo():
    """ë°°í¬ ì „ëµ ë°ëª¨"""
    print("\n=== ë°°í¬ ì „ëµ ë°ëª¨ ===\n")
    
    model_manager = ModelManager()
    
    # A/B í…ŒìŠ¤íŠ¸ ë°°í¬
    print("1. A/B í…ŒìŠ¤íŠ¸ ë°°í¬")
    
    # ëª¨ë¸ A (í˜„ì¬ ë²„ì „)
    version_a = await model_manager.train_and_register_model(
        model_id="pricing_model",
        model_type=ModelType.REGRESSION,
        train_data=None,
        params={'version': 'A', 'algorithm': 'RandomForest'}
    )
    
    # ëª¨ë¸ B (ì‹ ê·œ ë²„ì „)
    version_b = await model_manager.train_and_register_model(
        model_id="pricing_model",
        model_type=ModelType.REGRESSION,
        train_data=None,
        params={'version': 'B', 'algorithm': 'XGBoost'}
    )
    
    # ë‘ ëª¨ë¸ ë™ì‹œ ë°°í¬ (íŠ¸ë˜í”½ ë¶„í• )
    deployment_a = await model_manager.deploy_model(
        model_id="pricing_model",
        version=version_a,
        config={
            'deployment_type': 'api',
            'port': 8002,
            'traffic_percentage': 70  # 70% íŠ¸ë˜í”½
        }
    )
    
    deployment_b = await model_manager.deploy_model(
        model_id="pricing_model",
        version=version_b,
        config={
            'deployment_type': 'api',
            'port': 8003,
            'traffic_percentage': 30  # 30% íŠ¸ë˜í”½
        }
    )
    
    print(f"  âœ“ ëª¨ë¸ A (v{version_a}): 70% íŠ¸ë˜í”½")
    print(f"  âœ“ ëª¨ë¸ B (v{version_b}): 30% íŠ¸ë˜í”½")
    
    # ì¹´ë‚˜ë¦¬ ë°°í¬
    print("\n2. ì¹´ë‚˜ë¦¬ ë°°í¬")
    
    # ë‹¨ê³„ì  íŠ¸ë˜í”½ ì¦ê°€
    traffic_stages = [10, 25, 50, 100]
    
    for stage, traffic in enumerate(traffic_stages):
        print(f"  ìŠ¤í…Œì´ì§€ {stage+1}: {traffic}% íŠ¸ë˜í”½")
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ (ì‹œë®¬ë ˆì´ì…˜)
        error_rate = np.random.uniform(0.01, 0.03)  # 1-3% ì˜¤ë¥˜ìœ¨
        
        if error_rate > 0.025:  # ì„ê³„ê°’ ì´ˆê³¼
            print(f"    âš ï¸ ì˜¤ë¥˜ìœ¨ {error_rate:.1%} - ë¡¤ë°± í•„ìš”")
            await model_manager.deployer.rollback_deployment(deployment_b['deployment_id'])
            break
        else:
            print(f"    âœ“ ì˜¤ë¥˜ìœ¨ {error_rate:.1%} - ì •ìƒ")
        
        await asyncio.sleep(1)
    
    # ë¸”ë£¨-ê·¸ë¦° ë°°í¬
    print("\n3. ë¸”ë£¨-ê·¸ë¦° ë°°í¬")
    
    # í˜„ì¬ í”„ë¡œë•ì…˜ (ë¸”ë£¨)
    print("  ë¸”ë£¨ í™˜ê²½ (í˜„ì¬ í”„ë¡œë•ì…˜) í™œì„±")
    
    # ìƒˆ ë²„ì „ ì¤€ë¹„ (ê·¸ë¦°)
    print("  ê·¸ë¦° í™˜ê²½ ì¤€ë¹„ ì¤‘...")
    await asyncio.sleep(2)
    
    # ìŠ¤ìœ„ì¹˜
    print("  íŠ¸ë˜í”½ ì „í™˜: ë¸”ë£¨ â†’ ê·¸ë¦°")
    print("  âœ“ ë°°í¬ ì™„ë£Œ (ë‹¤ìš´íƒ€ì„ ì—†ìŒ)")


async def model_lifecycle_automation_demo():
    """ëª¨ë¸ ìƒëª…ì£¼ê¸° ìë™í™” ë°ëª¨"""
    print("\n=== ëª¨ë¸ ìƒëª…ì£¼ê¸° ìë™í™” ë°ëª¨ ===\n")
    
    model_manager = ModelManager({
        'email_alerts': False  # ì´ë©”ì¼ ì•Œë¦¼ ë¹„í™œì„±í™”
    })
    
    # ìë™ ì¬í•™ìŠµ ìŠ¤ì¼€ì¤„
    print("1. ìë™ ì¬í•™ìŠµ ìŠ¤ì¼€ì¤„ ì„¤ì •")
    
    from apscheduler.triggers.cron import CronTrigger
    
    async def auto_retrain():
        print(f"  [{datetime.now().strftime('%H:%M:%S')}] ìë™ ì¬í•™ìŠµ ì‹œì‘")
        version = await model_manager.train_and_register_model(
            model_id="auto_model",
            model_type=ModelType.TIME_SERIES,
            train_data=None,
            params={'auto_retrain': True}
        )
        print(f"  ìƒˆ ë²„ì „ {version} í•™ìŠµ ì™„ë£Œ")
    
    # ë§¤ì¼ ìƒˆë²½ 2ì‹œ ì¬í•™ìŠµ (ë°ëª¨ì—ì„œëŠ” ì¦‰ì‹œ ì‹¤í–‰)
    model_manager.scheduler.add_job(
        auto_retrain,
        CronTrigger(second='*/10'),  # ë°ëª¨: 10ì´ˆë§ˆë‹¤
        id='auto_retrain_job',
        max_instances=1
    )
    
    print("  âœ“ ìë™ ì¬í•™ìŠµ ìŠ¤ì¼€ì¤„ ë“±ë¡ (10ì´ˆë§ˆë‹¤)")
    
    # ëª¨ë¸ ì„±ëŠ¥ ê¸°ë°˜ ìë™ êµì²´
    print("\n2. ì„±ëŠ¥ ê¸°ë°˜ ìë™ ëª¨ë¸ êµì²´")
    
    current_model = "model_v1"
    new_model = "model_v2"
    
    # ì„±ëŠ¥ ë¹„êµ
    current_performance = 0.85
    new_performance = 0.88
    
    if new_performance > current_performance * 1.02:  # 2% ì´ìƒ ê°œì„ 
        print(f"  âœ“ ì‹ ê·œ ëª¨ë¸ ì„±ëŠ¥ {new_performance:.2%} > "
              f"í˜„ì¬ ëª¨ë¸ {current_performance:.2%}")
        print("  ìë™ìœ¼ë¡œ ì‹ ê·œ ëª¨ë¸ë¡œ êµì²´")
    else:
        print("  í˜„ì¬ ëª¨ë¸ ìœ ì§€")
    
    # ì˜¤ë˜ëœ ëª¨ë¸ ì •ë¦¬
    print("\n3. ì˜¤ë˜ëœ ëª¨ë¸ ìë™ ì •ë¦¬")
    model_manager.cleanup_old_models(days=30)
    print("  âœ“ 30ì¼ ì´ìƒ ëœ ì•„ì¹´ì´ë¸Œ ëª¨ë¸ ì •ë¦¬ ì™„ë£Œ")
    
    # 10ì´ˆ ëŒ€ê¸° (ìë™ ì¬í•™ìŠµ í™•ì¸)
    await asyncio.sleep(12)
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ì •ë¦¬
    model_manager.scheduler.remove_job('auto_retrain_job')


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ê¸°ë³¸ ëª¨ë¸ ìƒëª…ì£¼ê¸° ë°ëª¨
    await basic_model_lifecycle_demo()
    
    # ì‹¤í—˜ ì¶”ì  ë°ëª¨
    await experiment_tracking_demo()
    
    # ëª¨ë¸ ë²„ì „ ê´€ë¦¬ ë°ëª¨
    await model_versioning_demo()
    
    # ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ë°ëª¨
    await monitoring_alerts_demo()
    
    # ë°°í¬ ì „ëµ ë°ëª¨
    await deployment_strategies_demo()
    
    # ëª¨ë¸ ìƒëª…ì£¼ê¸° ìë™í™” ë°ëª¨
    await model_lifecycle_automation_demo()
    
    print("\n=== ëª¨ë“  ë°ëª¨ ì™„ë£Œ ===")


if __name__ == "__main__":
    asyncio.run(main())